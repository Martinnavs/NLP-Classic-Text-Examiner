{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project (taken from Codecademy's Natural Language Processing Course) aims to discover key insights into certain classical texts (the current example is James Joyce's novella The Dead) using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((('gabriel', 'NN'),), 37)\n",
      "((('i', 'NN'),), 26)\n",
      "((('o', 'NN'),), 8)\n",
      "((('face', 'NN'),), 8)\n",
      "((('the', 'DT'), ('window', 'NN')), 8)\n",
      "((('julia', 'NN'),), 7)\n",
      "((('kate', 'NN'),), 7)\n",
      "((('wife', 'NN'),), 7)\n",
      "((('a', 'DT'), ('moment', 'NN')), 7)\n",
      "((('overcoat', 'NN'),), 6)\n",
      "((('the', 'DT'), ('girl', 'NN')), 6)\n",
      "((('gretta', 'NN'),), 6)\n",
      "((('hand', 'NN'),), 5)\n",
      "((('mary', 'JJ'), ('jane', 'NN')), 4)\n",
      "((('conroy', 'NN'),), 4)\n",
      "((('aunt', 'NN'),), 4)\n",
      "((('the', 'DT'), ('pantry', 'NN')), 4)\n",
      "((('hat', 'NN'),), 4)\n",
      "((('body', 'NN'),), 4)\n",
      "((('hair', 'NN'),), 4)\n",
      "((('the', 'DT'), ('porter', 'NN')), 4)\n",
      "((('soul', 'NN'),), 4)\n",
      "((('voice', 'NN'),), 4)\n",
      "((('galway', 'NN'),), 4)\n",
      "((('mind', 'NN'),), 4)\n",
      "((('furey', 'NN'),), 4)\n",
      "((('miss', 'JJ'), ('kate', 'NN')), 3)\n",
      "((('miss', 'JJ'), ('julia', 'NN')), 3)\n",
      "((('dressing-room', 'NN'),), 3)\n",
      "((('life', 'NN'),), 3)\n",
      "\n",
      "\n",
      "\n",
      "((('i', 'NN'), ('was', 'VBD')), 7)\n",
      "((('i', 'NN'), ('am', 'VBP')), 2)\n",
      "((('i', 'NN'), ('used', 'VBN')), 2)\n",
      "((('the', 'DT'), ('wheezy', 'JJ'), ('hall-door', 'JJ'), ('bell', 'NN'), ('clanged', 'VBD'), ('again', 'RB')), 1)\n",
      "((('miss', 'JJ'), ('julia', 'NN'), ('had', 'VBD')), 1)\n",
      "((('the', 'DT'), ('bathroom', 'NN'), ('upstairs', 'VBZ')), 1)\n",
      "((('miss', 'JJ'), ('julia', 'NN'), ('were', 'VBD'), ('there', 'RB')), 1)\n",
      "((('life', 'NN'), ('was', 'VBD')), 1)\n",
      "((('miss', 'JJ'), ('julia', 'NN'), ('thought', 'VBN')), 1)\n",
      "((('julia', 'NN'), ('came', 'VBD')), 1)\n",
      "((('overcoat', 'NN'), ('slipped', 'VBD')), 1)\n",
      "((('gabriel', 'NN'), ('smiled', 'VBD')), 1)\n",
      "((('the', 'DT'), ('pantry', 'NN'), ('made', 'VBD')), 1)\n",
      "((('gabriel', 'NN'), ('had', 'VBD')), 1)\n",
      "((('step', 'NN'), ('nursing', 'VBG')), 1)\n",
      "((('i', 'NN'), (\"'m\", 'VBP')), 1)\n",
      "((('the', 'DT'), ('girl', 'NN'), ('glanced', 'VBD'), ('back', 'RB')), 1)\n",
      "((('gabriel', 'NN'), ('coloured', 'VBD')), 1)\n",
      "((('cheeks', 'NN'), ('pushed', 'VBD')), 1)\n",
      "((('glossy', 'JJ'), ('black', 'JJ'), ('hair', 'NN'), ('was', 'VBD')), 1)\n",
      "((('the', 'DT'), ('groove', 'NN'), ('left', 'VBN')), 1)\n",
      "((('culture', 'NN'), ('differed', 'VBN')), 1)\n",
      "((('whole', 'JJ'), ('speech', 'NN'), ('was', 'VBD')), 1)\n",
      "((('wife', 'NN'), ('came', 'VBD')), 1)\n",
      "((('julia', 'NN'), ('was', 'VBD')), 1)\n",
      "((('kate', 'NN'), ('was', 'VBD'), ('more', 'RBR')), 1)\n",
      "((('the', 'DT'), ('porter', 'NN'), ('halted', 'VBN')), 1)\n",
      "((('the', 'DT'), ('porter', 'NN'), ('led', 'VBD')), 1)\n",
      "((('the', 'DT'), ('porter', 'NN'), ('pointed', 'VBD')), 1)\n",
      "((('the', 'DT'), ('porter', 'NN'), ('took', 'VBD')), 1)\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, RegexpParser\n",
    "from tokenize_words import word_sentence_tokenize\n",
    "from chunks_counter import chunk_counter\n",
    "\n",
    "# import text of choice here\n",
    "text = open(\"the-dead.txt\",encoding='utf-8').read().lower()\n",
    "\n",
    "# sentence and word tokenize text here\n",
    "word_tokenized_text = word_sentence_tokenize(text)\n",
    "\n",
    "# store and print any word tokenized sentence here\n",
    "single_word_tokenized_sentence = word_tokenized_text[13]\n",
    "# print(single_word_tokenized_sentence)\n",
    "\n",
    "# create a list to hold part-of-speech tagged sentences here\n",
    "pos_tagged_text = []\n",
    "pos_tagged_text += [pos_tag(sen) for sen in word_tokenized_text]\n",
    "\n",
    "# create a for loop through each word tokenized sentence here\n",
    "\n",
    "  # part-of-speech tag each sentence and append to list of pos-tagged sentences here\n",
    "  \n",
    "\n",
    "# store and print any part-of-speech tagged sentence here\n",
    "single_pos_sentence = pos_tagged_text[13]\n",
    "# print(single_pos_sentence)\n",
    "\n",
    "# define noun phrase chunk grammar here\n",
    "chunk_grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "# chunk_grammar = \"\"\"NP: {<.*>+}\n",
    "#                   }<VB.?|IN>{\"\"\"\n",
    "\n",
    "# create noun phrase RegexpParser object here\n",
    "np_chunk_parser = RegexpParser(chunk_grammar)\n",
    "\n",
    "# define verb phrase chunk grammar here\n",
    "vp_chunk_grammar = \"VP: {<DT>?<JJ>*<NN><VB.*><RB.?>?}\"\n",
    "\n",
    "# create verb phrase RegexpParser object here\n",
    "vp_chunk_parser = RegexpParser(vp_chunk_grammar)\n",
    "\n",
    "#define a \n",
    "\n",
    "# create a list to hold noun phrase chunked sentences and a list to hold verb phrase chunked sentences here\n",
    "np_chunked_text = []\n",
    "vp_chunked_text = []\n",
    "\n",
    "\n",
    "# create a for loop through each pos-tagged sentence here\n",
    "for sen in pos_tagged_text:\n",
    "  # chunk each sentence and append to lists here\n",
    "  np_chunked_text.append(np_chunk_parser.parse(sen))\n",
    "  vp_chunked_text.append(vp_chunk_parser.parse(sen))\n",
    "\n",
    "# store and print the most common NP-chunks here\n",
    "most_common_np_chunks = chunk_counter(np_chunked_text,'NP')\n",
    "for i in most_common_np_chunks:\n",
    "    print(i)\n",
    "print(\"\\n\\n\")\n",
    "# store and print the most common VP-chunks here\n",
    "most_common_vp_chunks = chunk_counter(vp_chunked_text,'VP')\n",
    "for i in most_common_vp_chunks:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP NOUN PHRASES\n",
      "i was \n",
      "i am \n",
      "i used \n",
      "the wheezy hall-door bell clanged again \n",
      "miss julia had \n",
      "the bathroom upstairs \n",
      "miss julia were there \n",
      "life was \n",
      "miss julia thought \n",
      "julia came \n",
      "overcoat slipped \n",
      "gabriel smiled \n",
      "the pantry made \n",
      "gabriel had \n",
      "step nursing \n",
      "i 'm \n",
      "the girl glanced back \n",
      "gabriel coloured \n",
      "cheeks pushed \n",
      "glossy black hair was \n",
      "the groove left \n",
      "culture differed \n",
      "whole speech was \n",
      "wife came \n",
      "julia was \n",
      "kate was more \n",
      "the porter halted \n",
      "the porter led \n",
      "the porter pointed \n",
      "the porter took \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TOP VERB PHRASES\n",
      "i was \n",
      "i am \n",
      "i used \n",
      "the wheezy hall-door bell clanged again \n",
      "miss julia had \n",
      "the bathroom upstairs \n",
      "miss julia were there \n",
      "life was \n",
      "miss julia thought \n",
      "julia came \n",
      "overcoat slipped \n",
      "gabriel smiled \n",
      "the pantry made \n",
      "gabriel had \n",
      "step nursing \n",
      "i 'm \n",
      "the girl glanced back \n",
      "gabriel coloured \n",
      "cheeks pushed \n",
      "glossy black hair was \n",
      "the groove left \n",
      "culture differed \n",
      "whole speech was \n",
      "wife came \n",
      "julia was \n",
      "kate was more \n",
      "the porter halted \n",
      "the porter led \n",
      "the porter pointed \n",
      "the porter took \n"
     ]
    }
   ],
   "source": [
    "print(\"TOP NOUN PHRASES\")\n",
    "for chunk in most_common_vp_chunks:\n",
    "    for tups in chunk[0]:\n",
    "        print(tups[0], end = ' ')\n",
    "    print()\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"TOP VERB PHRASES\")\n",
    "for chunk in most_common_vp_chunks:\n",
    "    for tups in chunk[0]:\n",
    "        print(tups[0], end = ' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
